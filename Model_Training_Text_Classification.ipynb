{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJY7wS4cOdju"
      },
      "source": [
        "### Text Classfication using CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vbNaCNdiOdj0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import datetime\n",
        "import string\n",
        "import  nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import tree2conlltags\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Dropout,Input, Flatten, Embedding\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apSnEiS1O8G_"
      },
      "source": [
        "#### MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sXUpNNIrOdj-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('CNN_DOC_CLASSIFICATION_stacked_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW9r7_R-X-Nr",
        "outputId": "ab71552d-f713-488b-9fbc-1c88943a174b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-13 16:30:23--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-06-13 16:30:23--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-06-13 16:30:24--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.18MB/s    in 2m 40s  \n",
            "\n",
            "2022-06-13 16:33:04 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X_1MzF_YOdj-"
      },
      "outputs": [],
      "source": [
        "X = df['column_data_stacked']\n",
        "y = df['label']\n",
        "num_classes = len(set(list(y)))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MRxeh4aU5zP"
      },
      "source": [
        "####  Word-Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrsv47ykOdj_",
        "outputId": "e5bb3105-5040-4578-bacb-792845d7a252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(42363, 3865) (14121, 3865)\n"
          ]
        }
      ],
      "source": [
        "tk =  Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',)\n",
        "tk.fit_on_texts(X_train)\n",
        "vocab_size = len(tk.word_index) + 1\n",
        "encoded_train = tk.texts_to_sequences(X_train)\n",
        "encoded_test = tk.texts_to_sequences(X_test)\n",
        "\n",
        "max_len_train = max(([len(s.split()) for s in X_train]))\n",
        "max_len_test = max(([len(s.split()) for s in X_test]))\n",
        "\n",
        "X_tr = pad_sequences(encoded_train, maxlen=max_len_train, padding='post')\n",
        "X_te = pad_sequences(encoded_test, maxlen=max_len_test, padding='post')\n",
        "\n",
        "print(X_tr.shape,  X_te.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ziw7TglgOdj_"
      },
      "outputs": [],
      "source": [
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-BHoliH8OdkA"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tk.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B50z8RLrU-nB"
      },
      "source": [
        "####  Char-Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Hwfn4EOdkA",
        "outputId": "aaf9a218-ad70-45c6-9289-c59b5ca1bd02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(42363, 3865) (14121, 3865)\n"
          ]
        }
      ],
      "source": [
        "tk_char =  Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n', char_level=True, oov_token=True)\n",
        "tk_char.fit_on_texts(X_train)\n",
        "vocab_size_char= len(tk_char.word_index) + 1\n",
        "encoded_train = tk_char.texts_to_sequences(X_train)\n",
        "encoded_test = tk_char.texts_to_sequences(X_test)\n",
        "\n",
        "max_len_train = max(([len(s.split()) for s in X_train]))\n",
        "max_len_test = max(([len(s.split()) for s in X_test]))\n",
        "\n",
        "X_tr_char = pad_sequences(encoded_train, maxlen=max_len_train, padding='post')\n",
        "X_te_char = pad_sequences(encoded_test, maxlen=max_len_test, padding='post')\n",
        "\n",
        "# print(len(encoded_train),  encoded_test)\n",
        "print(X_tr_char.shape,  X_te_char.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4Gb8EFSsOdkB"
      },
      "outputs": [],
      "source": [
        "embeddings_index_char = dict()\n",
        "f = open('glove.840B.300d-char.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index_char[word] = coefs\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I4Py0DSvOdkB"
      },
      "outputs": [],
      "source": [
        "embedding_matrix_char = np.zeros((vocab_size_char, 300))\n",
        "for word, i in tk_char.word_index.items():\n",
        "\tembedding_vector_char = embeddings_index_char.get(word)\n",
        "\tif embedding_vector_char is not None:\n",
        "\t\tembedding_matrix_char[i] = embedding_vector_char"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL-tQyyYOdkB"
      },
      "source": [
        "#### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n-Kol6U2OdkC"
      },
      "outputs": [],
      "source": [
        "# Implementing ModelCheckpoint class which comes with keras library to save the learned weights if validation accuracy improves from previous epoch\n",
        "filepath = \"model_save/best_model-{epoch:02d}-{val_accuracy:.4f}.hdfs\"\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath = filepath,\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "__ot2KWvOdkD"
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.35, patience=2, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LtBvldOvOdkD"
      },
      "outputs": [],
      "source": [
        "tensorboard_log_dir = os.path.join(\"logs\", \"fits\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=tensorboard_log_dir,\n",
        "    histogram_freq=1,\n",
        "    write_graph=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBjuBjeVOdkD"
      },
      "source": [
        "#### Model -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b3y_Tv8wOdkE"
      },
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(vocab_size,\n",
        "                            100,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_len_train,\n",
        "                            trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cIDPcWqAOdkG"
      },
      "outputs": [],
      "source": [
        "def create_convnet(img_path='network_image.png'):\n",
        "    input_shape = Input(shape=(max_len_train))\n",
        "    embedded_sequences = embedding_layer(input_shape) \n",
        "    conv11 = Conv1D(filters=3, kernel_size=3, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(embedded_sequences)\n",
        "    conv12 = Conv1D(filters=5, kernel_size=5, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(embedded_sequences)\n",
        "    conv13 = Conv1D(filters=8, kernel_size=8, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(embedded_sequences)\n",
        "\n",
        "    merged1 = tf.keras.layers.concatenate([conv11, conv12, conv13])\n",
        "\n",
        "    pooling1 = MaxPooling1D(pool_size=3, padding='same')(merged1)\n",
        "\n",
        "    conv21 = Conv1D(filters=3, kernel_size=3, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(pooling1)\n",
        "    conv22 = Conv1D(filters=6, kernel_size=6, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(pooling1)\n",
        "    conv23 = Conv1D(filters=8, kernel_size=8, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(pooling1)\n",
        "\n",
        "    merged2 = tf.keras.layers.concatenate([conv21, conv22, conv23])\n",
        "    \n",
        "    pooling2 = MaxPooling1D(pool_size=3, padding='same')(merged2)\n",
        "\n",
        "    conv3 = Conv1D(filters=12, kernel_size=12, activation='relu', kernel_initializer=tf.keras.initializers.he_uniform())(pooling2)\n",
        "\n",
        "    Flatten1 = Flatten()(conv3)\n",
        "\n",
        "    dropout1 = Dropout(0.5)(Flatten1)\n",
        "\n",
        "    out_intm = Dense(20, activation='relu')(dropout1)\n",
        "    out = Dense(num_classes, activation='softmax')(out_intm)\n",
        "\n",
        "    model = Model(input_shape, out)\n",
        "    tf.keras.utils.plot_model(model, to_file=img_path)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU0lVIUpOdkG",
        "outputId": "839bf46b-b57e-4640-9c8f-ce8555f1a8fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 3865)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 3865, 100)    228400      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 3865, 3)      903         ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 3865, 5)      2505        ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 3865, 8)      6408        ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3865, 16)     0           ['conv1d[0][0]',                 \n",
            "                                                                  'conv1d_1[0][0]',               \n",
            "                                                                  'conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 1289, 16)     0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 1289, 3)      147         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 1289, 6)      582         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 1289, 8)      1032        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 1289, 17)     0           ['conv1d_3[0][0]',               \n",
            "                                                                  'conv1d_4[0][0]',               \n",
            "                                                                  'conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 430, 17)     0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 419, 12)      2460        ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 5028)         0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 5028)         0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 20)           100580      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 20)           420         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 343,437\n",
            "Trainable params: 115,037\n",
            "Non-trainable params: 228,400\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - ETA: 0s - loss: 2.2176 - accuracy: 0.3136 \n",
            "Epoch 1: val_accuracy improved from -inf to 0.65661, saving model to model_save/best_model-01-0.6566.hdfs\n",
            "INFO:tensorflow:Assets written to: model_save/best_model-01-0.6566.hdfs/assets\n",
            "22/22 [==============================] - 1188s 54s/step - loss: 2.2176 - accuracy: 0.3136 - val_loss: 1.0704 - val_accuracy: 0.6566\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.8781 - accuracy: 0.6814 \n",
            "Epoch 2: val_accuracy improved from 0.65661 to 0.78231, saving model to model_save/best_model-02-0.7823.hdfs\n",
            "INFO:tensorflow:Assets written to: model_save/best_model-02-0.7823.hdfs/assets\n",
            "22/22 [==============================] - 1202s 55s/step - loss: 0.8781 - accuracy: 0.6814 - val_loss: 0.4595 - val_accuracy: 0.7823\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.5586 - accuracy: 0.7658 \n",
            "Epoch 3: val_accuracy improved from 0.78231 to 0.80929, saving model to model_save/best_model-03-0.8093.hdfs\n",
            "INFO:tensorflow:Assets written to: model_save/best_model-03-0.8093.hdfs/assets\n",
            "22/22 [==============================] - 1212s 55s/step - loss: 0.5586 - accuracy: 0.7658 - val_loss: 0.4160 - val_accuracy: 0.8093\n",
            "Epoch 3: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d4fc14910>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# callback_list = [metric_callbacks_obj, early_stopping_callback, tensorboard_callback, checkpoint_callback]\n",
        "callback_list =[early_stopping_callback, tensorboard_callback, checkpoint_callback]\n",
        "\n",
        "model_1 = create_convnet()\n",
        "\n",
        "print(model_1.summary())\n",
        "\n",
        "optimizer_adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.01,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07\n",
        ")\n",
        "\n",
        "model_1.compile(optimizer=optimizer_adam, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model_1.fit(X_tr, y_train,  validation_data=(X_te, y_test),  epochs=10, batch_size=2000, callbacks=callback_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch2sZL8-OdkH"
      },
      "source": [
        "#### MODEL-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i1bM9e7gOdkH"
      },
      "outputs": [],
      "source": [
        "embedding_layer_model2 = Embedding(vocab_size_char,\n",
        "                            300,\n",
        "                            weights=[embedding_matrix_char],\n",
        "                            input_length=max_len_train,\n",
        "                            trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cGiIsRqNOdkI"
      },
      "outputs": [],
      "source": [
        "def create_convnet2(img_path='network_image_model2.png'):\n",
        "    input_shape = Input(shape=(max_len_train))\n",
        "    embedded_sequences = embedding_layer_model2(input_shape) \n",
        "    conv11 = Conv1D(filters=3, kernel_size=3, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(embedded_sequences)\n",
        "    conv12 = Conv1D(filters=5, kernel_size=5, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(conv11)\n",
        "\n",
        "    pooling1 = MaxPooling1D(pool_size=3, padding='same')(conv12)\n",
        "\n",
        "    conv21 = Conv1D(filters=6, kernel_size=6, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(pooling1)\n",
        "    conv22 = Conv1D(filters=8, kernel_size=8, activation='relu', padding ='same', kernel_initializer=tf.keras.initializers.he_uniform())(conv21)\n",
        "    \n",
        "    pooling2 = MaxPooling1D(pool_size=3, padding='same')(conv22)\n",
        "\n",
        "    Flatten1 = Flatten()(pooling2)\n",
        "\n",
        "    dropout1 = Dropout(0.5)(Flatten1)\n",
        "\n",
        "    out_intm = Dense(20, activation='relu')(dropout1)\n",
        "    out = Dense(num_classes, activation='softmax')(out_intm)\n",
        "\n",
        "    model = Model(input_shape, out)\n",
        "    tf.keras.utils.plot_model(model, to_file=img_path)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rssajrhOdkI",
        "outputId": "6d4c5c5b-6bbc-42c7-992c-752cd01e1e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 3865)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 3865, 300)         9900      \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 3865, 3)           2703      \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 3865, 5)           80        \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 1289, 5)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 1289, 6)           186       \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 1289, 8)           392       \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 430, 8)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 3440)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 3440)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                68820     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 82,501\n",
            "Trainable params: 72,601\n",
            "Non-trainable params: 9,900\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43/43 [==============================] - ETA: 0s - loss: 2.2999 - accuracy: 0.3332 \n",
            "Epoch 1: val_accuracy did not improve from 0.80929\n",
            "43/43 [==============================] - 785s 18s/step - loss: 2.2999 - accuracy: 0.3332 - val_loss: 1.6962 - val_accuracy: 0.5918\n",
            "Epoch 2/5\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.4415 - accuracy: 0.5595 \n",
            "Epoch 2: val_accuracy did not improve from 0.80929\n",
            "43/43 [==============================] - 784s 18s/step - loss: 1.4415 - accuracy: 0.5595 - val_loss: 0.9318 - val_accuracy: 0.7854\n",
            "Epoch 3/5\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9374 - accuracy: 0.6854 \n",
            "Epoch 3: val_accuracy did not improve from 0.80929\n",
            "43/43 [==============================] - 791s 18s/step - loss: 0.9374 - accuracy: 0.6854 - val_loss: 0.5193 - val_accuracy: 0.7982\n",
            "Epoch 3: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d43a5fb90>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "callback_list = [early_stopping_callback, tensorboard_callback, checkpoint_callback]\n",
        "\n",
        "model_2 = create_convnet2()\n",
        "print(model_2.summary())\n",
        "\n",
        "optimizer_adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07\n",
        ")\n",
        "\n",
        "model_2.compile(optimizer=optimizer_adam, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model_2.fit(X_tr_char, y_train, validation_data=(X_te_char, y_test),  epochs=5, batch_size=1000, callbacks=callback_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH4su1SHOdkJ"
      },
      "source": [
        "\n",
        "#### REFERENCE\n",
        "https://towardsdatascience.com/how-i-preprocessed-text-data-using-regular-expressions-for-my-text-classification-task-cnn-cb206e7274ed\n",
        "\n",
        "https://stackoverflow.com/questions/43151775/how-to-have-parallel-convolutional-layers-in-keras/\n",
        "\n",
        "http://ai.intelligentonlinetools.com/ml/document-classification-using-convolutional-neural-network/\n",
        "\n",
        "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "https://stackoverflow.com/questions/71357014/running-a-fine-tune-model-for-my-cnn-value-error"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "[Model Training]Text_Classification_Assign.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ae7824ee37ae96b6a633e4e9a37994fc39ffab56cd50d4823f9d2a0f50f1f477"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 ('py36_ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classfication using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import string\n",
    "import  nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Dropout,Input, Flatten, Embedding\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email(text_list):\n",
    "    # .* Zero or more characters of any type. \n",
    "    em = [] # for each doc\n",
    "    preprocess_email = \"\"  # for each doc\n",
    "    # temp=[]\n",
    "    #extracting email\n",
    "    emails_list = [email for email in re.findall(r'[\\w\\-\\.]+@[\\w\\.-]+\\b', text_list)]\n",
    "    for email in emails_list:\n",
    "        temp=[] #extracting email \n",
    "        str = \"\"\n",
    "        str += email.split('@')[1]      #taking text after @\n",
    "        temp = str.split('.')     #a list containing words split by \".\"\n",
    "        if 'com' in temp:\n",
    "            temp.remove('com')\n",
    "            \n",
    "        for word in temp:            #removing words less than 3\n",
    "            if len(word)>2:\n",
    "                em.append(word)\n",
    "                \n",
    "    for word in set(em):            #joining all the words in a string\n",
    "        preprocess_email+=word\n",
    "        preprocess_email+=' '\n",
    "    #removing the email\n",
    "    text_list =  re.sub(r'[\\w\\-\\.]+@[\\w\\.-]+\\b',' ', text_list)\n",
    "    return preprocess_email, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting subject\n",
    "def process_subject(text_list):\n",
    "    temp1 = re.findall(r'^Subject.*$',text_list, re.MULTILINE)\n",
    "    sub = temp1[0]\n",
    "    sub = sub[7:]   #Truncate Subject\n",
    "    for i in string.punctuation:   #remove all the non-alphanumeric\n",
    "        sub = sub.replace(i, \" \")\n",
    "        sub = re.sub(r\"re\",\"\", sub, flags=re.IGNORECASE) #removing Re\n",
    "        sub = re.sub(r\".*:\",\"\", sub, flags=re.IGNORECASE) #removing Re\n",
    "        sub = re.sub(r\"\\s+\",\" \", sub, flags=re.IGNORECASE) #removing Re\n",
    "        # sub = sub.lower()  #lower-casing\n",
    "    listy = re.sub(r'Subject.*$',\" \", text_list, flags=re.MULTILINE)\n",
    "    listy = re.sub(r\"Write to:.*$\",\" \", listy, flags=re.MULTILINE)           \n",
    "    listy = re.sub(r\"From:.*$\",\" \", listy, flags=re.MULTILINE)               \n",
    "    listy = re.sub(r\"or:\", \" \", listy,flags=re.MULTILINE)\n",
    "    listy = re.sub(r\"\\s+\",\" \", listy, flags=re.IGNORECASE) #removing Re\n",
    "    \n",
    "    return sub, listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(text_file):\n",
    "    chunks=[]\n",
    "    chunks=(list(ne_chunk(pos_tag(word_tokenize(text_file)))))\n",
    "   \n",
    "    for i in chunks:\n",
    "        if type(i)==nltk.tree.Tree:\n",
    "            if i.label() == \"GPE\":\n",
    "                j = i.leaves()\n",
    "                if len(j)>1:   #if new_delhi or bigger name\n",
    "                    gpe = \"_\".join([term for term, pos in j])\n",
    "                    text_file = re.sub(rf'{j[1][0]}',gpe, text_file, flags=re.MULTILINE)              #replacing delhi with new_delhi\n",
    "                    text_file = re.sub(rf'\\b{j[0][0]}\\b',\"\",text_file, flags=re.MULTILINE)       #deleting new, \\b is important\n",
    "            if i.label()==\"PERSON\":           # deleting Ramesh         \n",
    "                for term, pog in i.leaves():\n",
    "                    text_file = re.sub(re.escape(term),\"\",text_file, flags=re.MULTILINE)\n",
    "    return text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text_list):\n",
    " \n",
    "    # delete brackets\n",
    "    text_list = re.sub(r\"\\(.*?\\)\",\"\",text_list, flags=re.MULTILINE)            \n",
    "    text_list = re.sub(r\"<.*?>\",\"\", text_list, flags=re.MULTILINE) \n",
    "    # remove \"\\n\", \"\\t\", \"-\", \"\\\"\n",
    "    text_list = re.sub(r\"\\w+:\",\"\", text_list, flags=re.MULTILINE) #removing Ree\n",
    "    text_list = re.sub(r\"[\\n\\t\\-\\\\\\/]\",\" \", text_list, flags=re.MULTILINE)\n",
    "    text_list = re.sub(r'[~^0-9$]',\"\", text_list, flags=re.MULTILINE)\n",
    "    # decontration (from donor choose assignment)\n",
    "    # specific\n",
    "    text_list = re.sub(r\"won't\", \"will not\", text_list)\n",
    "    text_list = re.sub(r\"can\\'t\", \"can not\", text_list)\n",
    "    # general\n",
    "    text_list = re.sub(r\"n\\'t\", \" not\", text_list)\n",
    "    text_list = re.sub(r\"\\'re\", \" are\", text_list)\n",
    "    text_list = re.sub(r\"\\'s\", \" is\", text_list)\n",
    "    text_list = re.sub(r\"\\'d\", \" would\", text_list)\n",
    "    text_list = re.sub(r\"\\'ll\", \" will\", text_list)\n",
    "    text_list = re.sub(r\"\\'t\", \" not\", text_list)\n",
    "    text_list = re.sub(r\"\\'ve\", \" have\", text_list)\n",
    "    text_list = re.sub(r\"\\'m\", \" am\", text_list)\n",
    "    \n",
    "    text_list = re.sub(r\"\\b[a-zA-Z]{1,2}_([a-zA-Z]+)\",r\"\\1\",text_list) #d_berlin to berlin\n",
    "\n",
    "    text_list = re.sub(r\"\\b_([a-zA-z]+)_\\b\",r\"\\1\",text_list) #replace _word_ to word\n",
    "    text_list = re.sub(r\"\\b_([a-zA-z]+)\\b\",r\"\\1\",text_list) #replace_word to word\n",
    "    text_list = re.sub(r\"\\b([a-zA-z]+)_\\b\",r\"\\1\",text_list) #replace word_ to word\n",
    "\n",
    "    text_list =  chunking(text_list)\n",
    "    \n",
    "    text_list = re.sub(r'\\b\\w{1,2}\\b',\" \",text_list) #remove words <2\n",
    "    text_list = re.sub(r\"\\b\\w{15,}\\b\",\" \",text_list) #remove words >15\n",
    "    text_list = re.sub(r\"[^a-zA-Z_]\",\" \",text_list)  #keep only alphabets and _                                       \n",
    "    text_list = re.sub(r\" {2,}\", \" \", text_list, flags=re.MULTILINE) # REMOVE THE EXTRA SPACES\n",
    "\n",
    "    text_list = text_list.lower()\n",
    "    return text_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(Input_Text):\n",
    "  processed_text = []\n",
    "  processed_subject = []\n",
    "  processed_email = []\n",
    "  for i in range(len(Input_Text)):\n",
    "    p_email, Input_Text[i] = process_email(Input_Text[i])\n",
    "    p_subject, Input_Text[i] = process_subject(Input_Text[i])\n",
    "    p_Text = process_text(Input_Text[i])\n",
    "    processed_email.append(p_email)\n",
    "    processed_subject.append(p_subject)\n",
    "    processed_text.append(p_Text)\n",
    "\n",
    "  data['processed_email'] = processed_email\n",
    "  data['processed_subject'] = processed_subject\n",
    "  data['processed_text'] = processed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allLines = []\n",
    "labels = []\n",
    "filenum = []\n",
    "path = './documents/documents/'\n",
    "text_from_files = []\n",
    "fileList = os.listdir(path)\n",
    "for filename in fileList:\n",
    "    text_in_each_files= []\n",
    "    clas, num = filename.split('_')\n",
    "    labels.append(clas)\n",
    "    filenum.append(num.split('.')[0])\n",
    "    temp_file = open(os.path.join('./documents/documents/'+ filename), 'r+',\n",
    "     encoding=\"utf8\", errors='ignore')\n",
    "    text_from_files.append(temp_file.read())\n",
    "\n",
    "list_tuples = list(zip(labels, filenum))      \n",
    "data = pd.DataFrame(list_tuples, columns=['Label', 'File_num'])\n",
    "data.head()\n",
    " \n",
    "num_classes = len(set(list(labels)))  \n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>File_num</th>\n",
       "      <th>processed_email</th>\n",
       "      <th>processed_subject</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>49960</td>\n",
       "      <td>netcom mantis</td>\n",
       "      <td>Atheist sources</td>\n",
       "      <td>archive atheism resources alt atheism archive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>51060</td>\n",
       "      <td>mantis</td>\n",
       "      <td>Introduction to Atheism</td>\n",
       "      <td>rchive atheism introduction atheism archive i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>51119</td>\n",
       "      <td>edu mimsy umd dbstu1 tu-bs</td>\n",
       "      <td>Gospel Dating</td>\n",
       "      <td>article well has quite different not necessar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>51120</td>\n",
       "      <td>unh edu kepler mantis</td>\n",
       "      <td>university violating separation of church state</td>\n",
       "      <td>recently ras have been ordered post religious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>51121</td>\n",
       "      <td>Ibm org harder ccr-p Com Watson ibm watson ida</td>\n",
       "      <td>soc motss et al Princeton axes matching funds...</td>\n",
       "      <td>article however hate economic terrorism and p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label File_num                                  processed_email  \\\n",
       "0  alt.atheism    49960                                   netcom mantis    \n",
       "1  alt.atheism    51060                                          mantis    \n",
       "2  alt.atheism    51119                      edu mimsy umd dbstu1 tu-bs    \n",
       "3  alt.atheism    51120                           unh edu kepler mantis    \n",
       "4  alt.atheism    51121  Ibm org harder ccr-p Com Watson ibm watson ida    \n",
       "\n",
       "                                   processed_subject  \\\n",
       "0                                    Atheist sources   \n",
       "1                            Introduction to Atheism   \n",
       "2                                      Gospel Dating   \n",
       "3   university violating separation of church state    \n",
       "4   soc motss et al Princeton axes matching funds...   \n",
       "\n",
       "                                      processed_text  \n",
       "0   archive atheism resources alt atheism archive...  \n",
       "1   rchive atheism introduction atheism archive i...  \n",
       "2   article well has quite different not necessar...  \n",
       "3   recently ras have been ordered post religious...  \n",
       "4   article however hate economic terrorism and p...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text_from_files)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>column_name_stacked</th>\n",
       "      <th>column_data_stacked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>processed_email</td>\n",
       "      <td>netcom mantis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>processed_subject</td>\n",
       "      <td>Atheist sources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>processed_text</td>\n",
       "      <td>archive atheism resources alt atheism archive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>processed_email</td>\n",
       "      <td>netcom mantis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>processed_subject</td>\n",
       "      <td>Atheist sources</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label column_name_stacked  \\\n",
       "0      0     processed_email   \n",
       "1      0   processed_subject   \n",
       "2      0      processed_text   \n",
       "3      0     processed_email   \n",
       "4      0   processed_subject   \n",
       "\n",
       "                                 column_data_stacked  \n",
       "0                                     netcom mantis   \n",
       "1                                    Atheist sources  \n",
       "2   archive atheism resources alt atheism archive...  \n",
       "3                                     netcom mantis   \n",
       "4                                    Atheist sources  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en = LabelEncoder()\n",
    "df = pd.DataFrame(data.iloc[:, 2:5], en.fit_transform(data.iloc[:,0]), columns=['processed_email', 'processed_subject', 'processed_text', 'label']).stack().reset_index()\n",
    "df.rename(columns = {'level_0':'label', 'level_1':'column_name_stacked', 0:'column_data_stacked'}, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('CNN_DOC_CLASSIFICATION_stacked_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### REFERENCE\n",
    "https://towardsdatascience.com/how-i-preprocessed-text-data-using-regular-expressions-for-my-text-classification-task-cnn-cb206e7274ed\n",
    "\n",
    "https://stackoverflow.com/questions/43151775/how-to-have-parallel-convolutional-layers-in-keras/\n",
    "\n",
    "http://ai.intelligentonlinetools.com/ml/document-classification-using-convolutional-neural-network/\n",
    "\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "https://stackoverflow.com/questions/71357014/running-a-fine-tune-model-for-my-cnn-value-error"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae7824ee37ae96b6a633e4e9a37994fc39ffab56cd50d4823f9d2a0f50f1f477"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 ('py36_ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
